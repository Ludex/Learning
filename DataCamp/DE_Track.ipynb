{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql\n",
    "\n",
    "spark = pyspark.sql.SparkSession.builder.getOrCreate()\n",
    "spark.read.jdbc(\"jdbc:postgresql://localhost:5432/pagila\",\n",
    "                \"customer\",\n",
    "                properties = {\"user\"=\"repl\",\"password\"=\"password\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the rental rate column as a string\n",
    "rental_rate_str = film_df.rental_rate.astype(\"str\")\n",
    "\n",
    "# Split up and expand the column\n",
    "rental_rate_expanded = rental_rate_str.str.split(\".\", expand=True)\n",
    "\n",
    "# Assign the columns to film_df\n",
    "film_df = film_df.assign(\n",
    "    rental_rate_dollar = rental_rate_expanded[0],\n",
    "    rental_rate_cents = rental_rate_expanded[1],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow.models import DAG\n",
    "from airflow.operators.python_operator import PythonOperator\n",
    "\n",
    "dag = DAG(dag_id = \"sample\", schedule_interval = '0 * * *')\n",
    "\n",
    "etl_task = PythonOperator(\n",
    "    task_id = \"etl_task\",\n",
    "    python_callable = etl,\n",
    "    dag = dag\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
